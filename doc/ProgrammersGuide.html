<!DOCTYPE html PUBLIC "-//IETF//DTD HTML 2.0//EN">

<!--

Copyright 1999-2004 Carnegie Mellon University.
Portions Copyright 2004 Sun Microsystems, Inc.
Portions Copyright 2004 Mitsubishi Electric Research Laboratories.
All Rights Reserved.  Use is subject to license terms.

See the file "license.terms" for information on usage and
redistribution of this file, and for a DISCLAIMER OF ALL
WARRANTIES.

-->



<html>

<head>
  <title>Sphinx-4 Application Programmer's Guide</title>
   <style TYPE="text/css">
     pre { padding: 2mm; border-style: ridge; background: #f0f8ff; color: teal;}
     code {font-size: medium; color: teal}
   </style>
</head>

<body>
  <font face="Arial">
  <table bgcolor="#99CCFF" width="100%">
    <tr>
      <td align=center width="100%">
        <center><font face="Times New Roman"><h1>Sphinx-4 Application Programmer's Guide</h1></font></center>
      </td>
    </tr>
  </table>

  <p>
  This tutorial shows you how to write Sphinx-4 applications.
  We will use the Hello Digits
  demo as an example to show how a simple application can be written.
  We will then proceed to a more complex example. Consequently,
  this tutorial into the following parts:
  </p>

  <ol>
  <li><a href="#hellodigits">Simple Example - Hello Digits</a>
     <ul>
     <li><a href="#helloCodeWalk">Code Walk - HelloDigits.java</a></li>
     <li><a href="#helloConfigWalk">Configuration File Walk - hellodigits.config.xml</a>
       <ul>
         <li><a href="#recognizer">Recognizer</a></li>
	 <li><a href="#decoder">Decoder</a></li>
	 <li><a href="#linguist">Linguist</a></li>
	 <li><a href="#frontend">Front End</a></li>
	 <li><a href="#instrumentation">Instrumentation</a></li>
       </ul>
     </li>
     <li><a href="#buildFileWalk">Build File Walk - build.xml</a></li>
     </ul>
  </li>
  <br>
  <li><a href="#helloNGram">More Complex Example - Hello NGram</a>
      <ul>
          <li><a href="#ngramCodeWalk">Code Walk - HelloNGram.java</a></li>
      </ul>
  </li>
  </ol>

  <hr>  

  <h2><a name="hellodigits">1. Simple Example - Hello Digits</a></h2>

  <p>
  We will look at a very simple speech application written using
  Sphinx-4, our Hello Digits demo. This application recognizes connected
  digits. As you will see, the code is very simple. The tricky part is
  understanding the configuration, but we will guide you through every step
  of it. Lets look at the code first.
  </p>

  <h3><a name="helloCodeWalk">Code Walk - HelloDigits.java</a></h3>

  <p>
  All the source code of the Hello Digits demo is in one short file
  <code>sphinx4/demo/sphinx/hellodigits/HelloDigits.java</code>:
  </p>

  <pre>
/*
 * Copyright 1999-2004 Carnegie Mellon University.
 * Portions Copyright 2004 Sun Microsystems, Inc.
 * Portions Copyright 2004 Mitsubishi Electric Research Laboratories.
 * All Rights Reserved.  Use is subject to license terms.
 *
 * See the file "license.terms" for information on usage and
 * redistribution of this file, and for a DISCLAIMER OF ALL
 * WARRANTIES.
 *
 */

package demo.sphinx.hellodigits;

import edu.cmu.sphinx.frontend.util.Microphone;
import edu.cmu.sphinx.recognizer.Recognizer;
import edu.cmu.sphinx.result.Result;
import edu.cmu.sphinx.util.props.ConfigurationManager;
import edu.cmu.sphinx.util.props.PropertyException;

import java.io.File;
import java.io.IOException;
import java.net.URL;


/**
 * A simple HelloDigits demo showing a simple speech application 
 * built using Sphinx-4. This application uses the Sphinx-4 endpointer,
 * which automatically segments incoming audio into utterances and silences.
 */
public class HelloDigits {

    /**
     * Main method for running the HelloDigits demo.
     */
    public static void main(String[] args) {
        try {
            URL url;
            if (args.length > 0) {
                url = new File(args[0]).toURI().toURL();
            } else {
                url = HelloDigits.class.getResource("hellodigits.config.xml");
            }

            ConfigurationManager cm = new ConfigurationManager(url);

	    Recognizer recognizer = (Recognizer) cm.lookup("recognizer");
	    Microphone microphone = (Microphone) cm.lookup("microphone");


            /* allocate the resource necessary for the recognizer */
            recognizer.allocate();

            /* the microphone will keep recording until the program exits */
	    if (microphone.startRecording()) {

		System.out.println
		    ("Say any digit(s): e.g. \"two oh oh four\", " +
		         "\"three six five\".");

		while (true) {
		    System.out.println
			("Start speaking. Press Ctrl-C to quit.\n");

                    /*
		     * This method will return when the end of speech
		     * is reached. Note that the endpointer will determine
		     * the end of speech.
		     */ 
		    Result result = recognizer.recognize();
			     
	            if (result != null) {
			String resultText = result.getBestResultNoFiller();
			System.out.println("You said: " + resultText + "\n");
	            } else {
		        System.out.println("I can't hear what you said.\n");
	            }
		}
            } else {
	        System.out.println("Cannot start microphone.");
		recognizer.deallocate();
		System.exit(1);
            }
        } catch (IOException e) {
            System.err.println("Problem when loading HelloDigits: " + e);
            e.printStackTrace();
        } catch (PropertyException e) {
            System.err.println("Problem configuring HelloDigits: " + e);
            e.printStackTrace();
        } catch (InstantiationException e) {
            System.err.println("Problem creating HelloDigits: " + e);
            e.printStackTrace();
        }
    }
}
</pre>

  <br>
  This demo imports several important classes in Sphinx-4:
  <br>
  <code>
<br><a href="../javadoc/edu/cmu/sphinx/recognizer/Recognizer.html">edu.cmu.sphinx.recognizer.Recognizer</a>
<br><a href="../javadoc/edu/cmu/sphinx/result/Result.html">edu.cmu.sphinx.result.Result</a>
<br><a href="../javadoc/edu/cmu/sphinx/util/props/ConfigurationManager.html">edu.cmu.sphinx.util.props.ConfigurationManager</a>
  </code>
  
  <p>
  The <code>Recognizer</code> is the main class any application should
  interact with (refer also to the architecture diagram above).
  The <code>Result</code> is returned by the Recognizer to the application
  after recognition completes. The <code>ConfigurationManager</code>
  creates the entire Sphinx-4 system according to the configuration specified
  by the user.
  </p>
  
  <p>
  Lets look at the <code>main()</code> method. The first few
  lines creates the URL of the XML-based configuration file.
  A <code>ConfigurationManager</code> is then created using that URL.
  The ConfigurationManager then reads in the
  file internally. Since the configuration file specifies the components 
  <code>"recognizer"</code> and <code>"microphone"</code> (we will look
  at the configuration file next),
  we perform a <a href="../javadoc/edu/cmu/sphinx/util/props/ConfigurationManager.html#lookup(java.lang.String)"><code>lookup()</code></a> of these components
  in the ConfigurationManager to obtain these components. The <a href="../javadoc/edu/cmu/sphinx/recognizer/Recognizer.html#allocate()"><code>allocate()</code></a> method of the Recognizer is then called to allocate the resources
  need for the recognizer. The Microphone class is used for capturing live
  audio from the system audio device.
  </p>

  <p>
  We then create a thread that listens to the ENTER key (ASCII value 10).
  Whenever the ENTER key is pressed, the method 
  <code>microphone.stopRecording()</code> will be called.
  </p>
  
  <p>
  Once all the necessary components are created, we can start running the demo.
  The program enters a loop that repeats the following.
  The program first turns on the Microphone 
  (<a href="../javadoc/edu/cmu/sphinx/frontend/util/Microphone.html#startRecording()"><code>microphone.startRecording()</code></a>).
  After the microphone is turned on successfully, tries to recognize what 
  the user is saying, using the
  <a href="../javadoc/edu/cmu/sphinx/recognizer/Recognizer.html#recognize()"><code>Recognizer.recognize()</code></a> method. Recognition stops when
  the user hits the ENTER key, at which point the microphone is turned off,
  and recognition stops when there is no more data left.
  Once an utterance is recognized, the recognized text,
  which is returned by the method
  <a href="../javadoc/edu/cmu/sphinx/result/Result.html#getBestResultNoFiller()"><code>Result.getBestResultNoFiller()</code></a>, is printed out. 
  If the Recognizer recognized nothing (i.e., <code>result</code> is
  null), then it will print out a message saying that. Finally, if the
  demo program cannot turn on the microphone in the first place, the
  Recognizer will be deallocated, and the program exits.
  It is generally a good practice to call the method
  <a href="../javadoc/edu/cmu/sphinx/recognizer/Recognizer.html#deallocate()"><code>deallocate()</code></a> after the work is done to release all the
  resources.
  </p>
  
  <p>
  Note that several exceptions were being caught. First of all,
  the IOException is thrown by the <a href="../javadoc/edu/cmu/sphinx/util/props/ConfigurationManager.html#ConfigurationManager(java.net.URL)">constructor</a>
  of the ConfigurationManager and the Recognizer.allocate() method.
  The <a href="../javadoc/edu/cmu/sphinx/util/props/PropertyException.html">PropertyException</a> 
  is thrown again by the constructor, and by the
  <code>lookup()</code> method, of the ConfigurationManager.
  These exceptions should be caught and handled appropriately.
  </p>

  <p>
  Hopefully, by this point, you will have some idea of how to write a simple
  Sphinx-4 application. We will now turn to the harder part, understanding
  the various components necessary to create a connected-digits recognizer.
  These components are specified in the configuration file, which we will
  now explain in depth.
  </p>

  <h3><a name="helloConfigWalk">Configuration File Walk - hellodigits.config.xml</a></h3>

  In this section, we will explain the various Sphinx-4 components that
  are used for the Hello Digits demo, as specified in the configuration file.
  We will look at each section of the config file in depth. If you want
  to learn about the format of these configuration files, please refer
  to the document <a href="../javadoc/edu/cmu/sphinx/util/props/doc-files/ConfigurationManagement.html">Sphinx-4 Configuration Management</a>.

<pre>
    &lt;!-- ******************************************************** --&gt;
    &lt;!-- frequently tuned properties                              --&gt;
    &lt;!-- ******************************************************** --&gt; 

    &lt;property name="logLevel" value="WARNING"/&gt;
    
    &lt;property name="absoluteBeamWidth"  value="-1"/&gt;
    &lt;property name="relativeBeamWidth"  value="1E-80"/&gt;
    &lt;property name="wordInsertionProbability" value="1E-36"/&gt;
    &lt;property name="languageWeight"     value="8"/&gt;
    
    &lt;property name="frontend" value="epFrontEnd"/&gt;
    &lt;property name="recognizer" value="recognizer"/&gt;
    &lt;property name="showCreations" value="false"/&gt;
</pre>

The above lines defines frequently tuned properties. They are located at the
top of the configuration file so that they can be edited quickly.

<h4><a name="recognizer">Recognizer</a></h4>

<pre>
    &lt;!-- ******************************************************** --&gt;
    &lt;!-- word recognizer configuration                            --&gt;
    &lt;!-- ******************************************************** --&gt; 
    
    &lt;component name="recognizer" type="edu.cmu.sphinx.recognizer.Recognizer"&gt;
        &lt;property name="decoder" value="decoder"/&gt;
        &lt;propertylist name="monitors"&gt;
            &lt;item&gt;accuracyTracker &lt;/item&gt;
            &lt;item&gt;speedTracker &lt;/item&gt;
            &lt;item&gt;memoryTracker &lt;/item&gt;
        &lt;/propertylist&gt;
    &lt;/component&gt;
</pre>

The above lines define the recognizer component that performs speech
recognition. It defines the name and class of the recognizer. 
This is the class that any application should interact with.
If you look at the <a href="../javadoc/edu/cmu/sphinx/recognizer/Recognizer.html">javadoc of the Recognizer class</a>, you will see that it has two
properties, 'decoder' and 'monitors'. This configuration file is where
the value of these properties are defined.

<h4><a name="decoder">Decoder</a></h4>

The 'decoder' property of the recognizer is set to the component 
called 'decoder':

<pre>
    &lt;component name="decoder" type="edu.cmu.sphinx.decoder.Decoder"&gt;
        &lt;property name="searchManager" value="searchManager"/&gt;
    &lt;/component&gt;
</pre>

The decoder component is defined to be of class
<code><a href="../javadoc/edu/cmu/sphinx/decoder/Decoder.html">edu.cmu.sphinx.decoder.Decoder</a></code>. Its property 'searchManager'
is set to the component 'searchManager':

<pre>
    &lt;component name="searchManager" 
        type="edu.cmu.sphinx.decoder.search.SimpleBreadthFirstSearchManager"&gt;
        &lt;property name="logMath" value="logMath"/&gt;
        &lt;property name="linguist" value="flatLinguist"/&gt;
        &lt;property name="pruner" value="trivialPruner"/&gt;
        &lt;property name="scorer" value="threadedScorer"/&gt;
        &lt;property name="activeListFactory" value="activeList"/&gt;
    &lt;/component&gt;
</pre>

The searchManager is of class
<code><a href="../javadoc/edu/cmu/sphinx/decoder/search/SimpleBreadthFirstSearchManager.html">edu.cmu.sphinx.decoder.search.SimpleBreadthFirstSearchManager</a></code>.
This class performs a simple breadth-first search through the search graph
during the decoding process to find the best path. This search manager 
is suitable for small to medium sized vocabulary decoding.
The logMath property is the
log math that is used for calculation of scores during the search process.
It is defined as having the log base of 1.0001. Note that typically the
same log base should be used throughout all components, and therefore
there should only be one logMath definition:

<pre>
    &lt;component name="logMath" type="edu.cmu.sphinx.util.LogMath"&gt;
        &lt;property name="logBase" value="1.0001"/&gt;
        &lt;property name="useAddTable" value="true"/&gt;
    &lt;/component&gt;
</pre>

The linguist of the searchManager is set to the component 'flatLinguist' 
(which we will look at later), which again is suitable for small to medium 
sized vocabulary decoding. The pruner is set to the 'trivialPruner':

<pre>
    &lt;component name="trivialPruner" 
                type="edu.cmu.sphinx.decoder.pruner.SimplePruner"/&gt;
</pre>

which is of class <code><a href="../javadoc/edu/cmu/sphinx/decoder/pruner/SimplePruner.html">edu.cmu.sphinx.decoder.pruner.SimplePruner</a></code>.
This pruner performs simple absolute beam and relative beam pruning
based on the scores of the tokens.

The scorer of the searchManager is set to the component 'threadedScorer',
which is of class
<code><a href="../javadoc/edu/cmu/sphinx/decoder/scorer/ThreadedAcousticScorer.html">edu.cmu.sphinx.decoder.scorer.ThreadedAcousticScorer</a></code>.
It can use multiple threads (usually one per CPU) to score the tokens 
in the active list. Scoring is one of the most time-consuming step of the
decoding process. Tokens can be scored independently of each other, 
so using multiple CPUs will definitely speed things up.
The threadedScorer is defined as follows:

<pre>
    &lt;component name="threadedScorer" 
                type="edu.cmu.sphinx.decoder.scorer.ThreadedAcousticScorer"&gt;
        &lt;property name="frontend" value="${frontend}"/&gt;
        &lt;property name="isCpuRelative" value="true"/&gt;
        &lt;property name="numThreads" value="0"/&gt;
        &lt;property name="minScoreablesPerThread" value="10"/&gt;
        &lt;property name="scoreablesKeepFeature" value="true"/&gt;
    &lt;/component&gt;
</pre>

The 'frontend' property is the front end from which features are obtained.
For details about the other properties of the threadedScorer, please refer to
<a href="../javadoc/edu/cmu/sphinx/decoder/scorer/ThreadedAcousticScorer.html">javadoc for ThreadedAcousticScorer</a>.

Finally, the activeListFactory property of the searchManager is set to
the component 'activeList', which is defined as follows:

<pre>
    &lt;component name="activeList" 
             type="edu.cmu.sphinx.decoder.search.PartitionActiveListFactory"&lt;
        &lt;property name="logMath" value="logMath"/&lt;
        &lt;property name="absoluteBeamWidth" value="${absoluteBeamWidth}"/&lt;
        &lt;property name="relativeBeamWidth" value="${relativeBeamWidth}"/&lt;
    &lt;/component&lt;
</pre>

It is of class <code><a href="../javadoc/edu/cmu/sphinx/decoder/search/PartitionActiveListFactory.html">edu.cmu.sphinx.decoder.search.PartitionActiveListFactory</a></code>. 
It uses a partitioning algorithm to select the top N highest scoring
tokens when performing absolute beam pruning. The 'logMath' property
specifies the log math used for score calculation, which is the same
LogMath used in the searchManager. The property 'absoluteBeamWidth'
is set to the value given at the very top of the configuration file using
<code>${absoluteBeamWidth}</code>. The same is for 'relativeBeamWidth'.

<h4><a name="linguist">Linguist</a></h4>

Now lets look at the flatLinguist component (a component inside the
searchManager). The linguist is the component that generates the search
graph using the guidance from the grammar, and knowledge from the dictionary,
acoustic model, and language model.

<pre>
    &lt;!-- ******************************************************** --&gt;
    &lt;!-- The linguist  configuration                              --&gt;
    &lt;!-- ******************************************************** --&gt;
    
    &lt;component name="flatLinguist" 
                type="edu.cmu.sphinx.linguist.flat.FlatLinguist"&gt;
        &lt;property name="logMath" value="logMath"/&gt;
        &lt;property name="grammar" value="jsgfGrammar"/&gt;
        &lt;property name="acousticModel" value="wsj"/&gt;
        &lt;property name="wordInsertionProbability" 
                value="${wordInsertionProbability}"/&gt;
        &lt;property name="languageWeight" value="${languageWeight}"/&gt;
    &lt;/component&gt;
</pre>

It uses the log math that we've seen already. The grammar used is the
component called 'jsgfGrammar', which is a BNF-style grammar:

<pre>
    &lt;component name="jsgfGrammar" type="edu.cmu.sphinx.jsapi.JSGFGrammar"&gt;
        &lt;property name="grammarLocation" value="/demo/sphinx/hellodigits"/&gt;
        &lt;property name="dictionary" value="dictionary"/&gt;
        &lt;property name="grammarName" value="digits"/&gt;
	&lt;property name="logMath" value="logMath"/&gt;
    &lt;/component&gt;
</pre>

JSGF grammars are defined in <a href="http://java.sun.com/products/java-media/speech/">JSAPI</a>. The class that translates JSGF into a form that 
Sphinx-4 understands is <code>
<a href="../javadoc/edu/cmu/sphinx/jsapi/JSGFGrammar.html">edu.cmu.sphinx.jsapi.JSGFGrammar</a></code>.
The property 'grammarLocation' can take two kinds of values. 
If it is a URL, it specifies the URL of the directory where JSGF grammar files 
are to be found. Otherwise, it is interpreted as resource locator. 
In our example, the HelloDigits demo is being deployed as a JAR file.
The 'grammarLocation' property is therefore used to specify 
the location of the resource "digits.gram" within the JAR file. Note that 
it is not necessary to name the JAR file within which to search. The system 
searches all the relevant JAR files. The 'grammarName' property 
specifies the grammar to use when creating the search graph. 'logMath' is the 
same log math as the other components. The 'dictionary' is the component
that maps words to their phonemes. It is almost always the dictionary of the
acoustic model, which lists all the words that were used to trained 
the acoustic model:

<pre>
    &lt;component name="dictionary" 
        type="edu.cmu.sphinx.linguist.dictionary.FastDictionary"&gt;
        &lt;property name="location" 
                value="file:./../../../models/acoustic/wsj_8gau_13dCep_16k_40mel_130Hz_6800Hz.bin.zip"/&gt;
        &lt;property name="dictionaryPath" value= "dict/cmudict.0.6d"/&gt;
        &lt;property name="fillerPath" value="dict/fillerdict"/&gt;
        &lt;property name="addSilEndingPronunciation" value="false"/&gt;
        &lt;property name="wordReplacement" value="&lt;sil&gt;"/&gt;
        &lt;property name="allowMissingWords" value="true"/&gt;
    &lt;/component&gt;
</pre>

As you can see, it is using the dictionary inside the Wall Street journal
acoustic model. The main dictionary for words is
<code>dict/cmudict.0.6d</code>, and the dictionary for filler words
like "BREATH" and "LIP_SMACK" is <code>dict/fillerdict</code>. 
For details about the other properties, please refer to the
<a href="../javadoc/edu/cmu/sphinx/linguist/dictionary/FastDictionary.html">javadoc for FastDictionary</a>.

<p>
The next important property of the flatLinguist is the acoustic model,
which is defined as:
</p>

<pre>
    &lt;component name="wsj" 
      type="edu.cmu.sphinx.linguist.acoustic.tiedstate.TiedStateAcousticModel"&gt;
        &lt;property name="loader" value="sphinx3Loader"/&gt;
    &lt;/component&gt;
    
    &lt;component name="sphinx3Loader" 
           type="edu.cmu.sphinx.linguist.acoustic.tiedstate.Sphinx3Loader"&gt;
        &lt;property name="logMath" value="logMath"/&gt;
        &lt;property name="isBinary" value="true"/&gt;
        &lt;property name="location" 
                value="file:./../../../models/acoustic/wsj_8gau_13dCep_16k_40mel_130Hz_6800Hz.bin.zip"/&gt;
        &lt;property name="definition_file" 
                value="etc/WSJ_clean_13dCep_16k_40mel_130Hz_6800Hz.4000.mdef"/&gt;
        &lt;property name="data_location" 
                value="cd_continuous_8gau"/&gt;
        &lt;property name="properties_file" value="am.props"/&gt;
    &lt;/component&gt;
</pre>

The component 'wsj', the Wall Street Journal (WSJ) acoustic model is defined
above. The WSJ models, which contains about 129000 words, are usually 
suitable for general purpose speech recognition. It is a tied-state acoustic
model, and because the models are arranged in Sphinx-3 format, it is loaded
by the <code><a href="../javadoc/edu/cmu/sphinx/linguist/acoustic/tiedstate/Sphinx3Loader.html">Sphinx3Loader</a></code>. The loader uses the same logMath as
all other components. The format of the model is binary, and the
URL location of the model is specified. For details about the other
properties, please refer to the <a href="../javadoc/edu/cmu/sphinx/linguist/acoustic/tiedstate/Sphinx3Loader.html">javadoc for Sphinx3Loader</a>.

The next properties of the flatLinguist are the 'wordInsertionProbability'
and 'languageWeight'. These properties are usually for fine tuning the
system. Below are the default values we used for the various tasks.
You can tune your system accordingly:

<p>
<table width="100%">
<tr><td><b>Vocabulary Size</b></td><td><b>Word Insertion Probability</b></td><td><b>Language Weight</b></td></tr>
<tr><td>Digits (11 words - TIDIGITS)</td><td>1E-36</td><td>8</td></tr>
<tr><td>Small (80 words - AN4)</td><td>1E-26</td><td>7</td></tr>
<tr><td>Medium (1000 words - RM1)</td><td>1E-10</td><td>7</td></tr>
<tr><td>Large (64000 words - HUB4)</td><td>0.2</td><td>10.5</td></tr>
</table>
</p>

<h4><a name="frontend">Front End</a></h4>

The last big piece in the configuration file is the front end. There are 
two different front ends listed in the configuration file: 'frontend' and
'epFrontEnd'. The 'frontend' is good for batch mode decoding (or decoding
without endpointing), while 'epFrontEnd' is good for live mode decoding
with endpointing. Note that you can also perform live mode decoding
with the 'frontend' (i.e., without endpointing), but that you need to
explicitly signal the start and end of speech (e.g., by asking the user
to explicitly turn on/off the microphone). 
The definitions for these front ends are:

<pre>
    &lt;!-- ******************************************************** --&gt;
    &lt;!-- The frontend configuration                               --&gt;
    &lt;!-- ******************************************************** --&gt;
    
    &lt;component name="frontEnd" type="edu.cmu.sphinx.frontend.FrontEnd"&gt;
        &lt;propertylist name="pipeline"&gt;
            &lt;item&gt;microphone &lt;/item&gt;
            &lt;item&gt;premphasizer &lt;/item&gt;
            &lt;item&gt;windower &lt;/item&gt;
            &lt;item&gt;fft &lt;/item&gt;
            &lt;item&gt;melFilterBank &lt;/item&gt;
            &lt;item&gt;dct &lt;/item&gt;
            &lt;item&gt;liveCMN &lt;/item&gt;
            &lt;item&gt;featureExtraction &lt;/item&gt;
        &lt;/propertylist&gt;
    &lt;/component&gt;
    
    &lt;!-- ******************************************************** --&gt;
    &lt;!-- The live frontend configuration                          --&gt;
    &lt;!-- ******************************************************** --&gt;
    &lt;component name="epFrontEnd" type="edu.cmu.sphinx.frontend.FrontEnd"&gt;
        &lt;propertylist name="pipeline"&gt;
            &lt;item&gt;microphone &lt;/item&gt;
            &lt;item&gt;speechClassifier &lt;/item&gt;
            &lt;item&gt;speechMarker &lt;/item&gt;
            &lt;item&gt;nonSpeechDataFilter &lt;/item&gt;
            &lt;item&gt;premphasizer &lt;/item&gt;
            &lt;item&gt;windower &lt;/item&gt;
            &lt;item&gt;fft &lt;/item&gt;
            &lt;item&gt;melFilterBank &lt;/item&gt;
            &lt;item&gt;dct &lt;/item&gt;
            &lt;item&gt;liveCMN &lt;/item&gt;
            &lt;item&gt;featureExtraction &lt;/item&gt;
        &lt;/propertylist&gt;
    &lt;/component&gt;
</pre>

As you might notice, the only different between these two front ends is
that the live front end (epFrontEnd) has the additional components
<code>speechClassifier</code>, <code>speechMarker</code> and
<code>nonSpeechDataFilter</code>. These three components make up the
default endpointer of Sphinx-4. Below is a listing of all the components
of both front ends, and those properties which have values different from
the default:

<pre>

    &lt;component name="speechClassifier" 
               type="edu.cmu.sphinx.frontend.endpoint.SpeechClassifier"&gt;
        &lt;property name="threshold" value="13"/&gt;
    &lt;/component&gt;
    
    &lt;component name="nonSpeechDataFilter" 
               type="edu.cmu.sphinx.frontend.endpoint.NonSpeechDataFilter"/&gt;
    
    &lt;component name="speechMarker" 
               type="edu.cmu.sphinx.frontend.endpoint.SpeechMarker" &gt;
        &lt;property name="speechTrailer" value="50"/&gt;
    &lt;/component&gt;
    
    
    &lt;component name="premphasizer" 
               type="edu.cmu.sphinx.frontend.filter.Preemphasizer"/&gt;
    
    &lt;component name="windower" 
               type="edu.cmu.sphinx.frontend.window.RaisedCosineWindower"&gt;
    &lt;/component&gt;
    
    &lt;component name="fft" 
            type="edu.cmu.sphinx.frontend.transform.DiscreteFourierTransform"/&gt;
    
    &lt;component name="melFilterBank" 
        type="edu.cmu.sphinx.frontend.frequencywarp.MelFrequencyFilterBank"&gt;
    &lt;/component&gt;
    
    &lt;component name="dct" 
            type="edu.cmu.sphinx.frontend.transform.DiscreteCosineTransform"/&gt;
    
    &lt;component name="batchCMN" 
               type="edu.cmu.sphinx.frontend.feature.BatchCMN"/&gt;

    &lt;component name="liveCMN" 
               type="edu.cmu.sphinx.frontend.feature.LiveCMN"/&gt;
        
    &lt;component name="featureExtraction" 
               type="edu.cmu.sphinx.frontend.feature.DeltasFeatureExtractor"/&gt;
       
    &lt;component name="microphone" 
               type="edu.cmu.sphinx.frontend.util.Microphone"&gt;
        &lt;property name="bytesPerRead" value="320"/&gt;
        &lt;property name="closeBetweenUtterances" value="false"/&gt;
    &lt;/component&gt;
</pre>

Lets explain some of the properties set here that have values different
from the default. The property <a href="../javadoc/edu/cmu/sphinx/frontend/endpoint/SpeechClassifier.html#PROP_THRESHOLD">'threshold'</a> of the 
SpeechClassifier specifies the minimum difference between the input signal
level and the background signal level in order that the input signal is
classified as speech. Therefore, the smaller this number, the more sensitive
the endpointer, and vice versa. The property <a href="../javadoc/edu/cmu/sphinx/frontend/endpoint/SpeechMarker.html#PROP_SPEECH_TRAILER">'speechTrailer'</a> of the SpeechMarker specifies the length of non-speech signal to be included after the end of speech to make sure that no speech signal is lost. Here, it is
set at 50 milliseconds. The property <a href="../javadoc/edu/cmu/sphinx/frontend/util/Microphone.html#PROP_BYTES_PER_READ">'bytesPerRead'</a> of the
Microphone specifies the number of bytes of each read from the system audio
device. The value specified here is 320 bytes, which is 160 samples given a
sample size of 2 bytes. The property
<a href="../javadoc/edu/cmu/sphinx/frontend/util/Microphone.html#PROP_CLOSE_BETWEEN_UTTERANCES">'closeBetweenUtterances'</a> specifies whether the system
audio device should be released between utterances. It is set to false here,
meaning that the system audio device will not be released between utterances.
This is set as so because on certain systems (Linux for one), closing and 
reopening the audio does not work too well.


<h4><a name="instrumentation">Instrumentation</a></h4>

Finally, we will explain the various monitors which make up the 
<a href="../javadoc/edu/cmu/sphinx/instrumentation/package-summary.html">instrumentation package</a>. These monitors are components
of the recognizer (see above). They are responsible for tracking the 
accuracy, speed and memory usage of Sphinx-4. 

<pre>
    &lt;component name="accuracyTracker" 
                type="edu.cmu.sphinx.instrumentation.AccuracyTracker"&gt;
        &lt;property name="recognizer" value="${recognizer}"/&gt;
        &lt;property name="showAlignedResults" value="false"/&gt;
        &lt;property name="showRawResults" value="false"/&gt;
    &lt;/component&gt;
    
    &lt;component name="memoryTracker" 
                type="edu.cmu.sphinx.instrumentation.MemoryTracker"&gt;
        &lt;property name="recognizer" value="${recognizer}"/&gt;
	&lt;property name="showSummary" value="false"/&gt;
	&lt;property name="showDetails" value="false"/&gt;
    &lt;/component&gt;
    
    &lt;component name="speedTracker" 
                type="edu.cmu.sphinx.instrumentation.SpeedTracker"&gt;
        &lt;property name="recognizer" value="${recognizer}"/&gt;
        &lt;property name="frontend" value="${frontend}"/&gt;
	&lt;property name="showSummary" value="true"/&gt;
	&lt;property name="showDetails" value="false"/&gt;
    &lt;/component&gt;
</pre>

The various knobs of these monitors mainly control whether statistical
information about accuracy, speed and memory usage should be printed out.
Moreover, the monitors monitor the behavior of a recognizer,
so they need a reference to the recognizer that they are monitoring.


<p><h3><a name="buildFileWalk">Build File Walk - build.xml</a></h3></p>

The last piece of the puzzle of the Hello Digits demo is the build.xml
file, which defines Ant targets for running the demo. For details about
writing the build.xml file, please refer to the 
<a href="http://ant.apache.org/manual/index.html">Ant documentation</a>.
The file <code>demo/sphinx/hellodigits/build.xml</code> is pretty
straightforward. Lets look at the top part of the build.xml file:

<pre>
    &lt;property name="top_dir"		value="../../.."/&gt;
    &lt;property name="build_dir"		value="${top_dir}/bld"/&gt;
    &lt;property name="classes_dir"	value="${build_dir}/classes"/&gt;
    &lt;property name="lib_dir"            value="${build_dir}/lib"/&gt;

    &lt;path id="run.classpath"&gt;
        &lt;pathelement path="${classes_dir}"/&gt;
	&lt;pathelement location="${top_dir}/lib/jsapi.jar"/&gt;
    &lt;/path&gt;
</pre>

It defines several variables, e.g., the top level Sphinx-4 directory,
the build directory, the classes directory, and the libraries directory.
It also defines the <codE>run.classpath</code> path ID, which includes
the classes directory and the <code>jsapi.jar</code> library, the latter of
which is needed for interpreting JSGF grammars in the Hello Digits demo.

<p>Now lets take a look at the "run" target:</p>

<pre>
    &lt;target name="run"
	    description="Runs the hello world demo."
	    depends="all"&gt;
	    &lt;java classname="demo.sphinx.hellodigits.HelloDigits"
	          fork="true"
		  maxmemory="128m"&gt;
		  &lt;classpath refid="run.classpath"/&gt;
		  &lt;arg value="hellodigits.config.xml"/&gt;
	    &lt;/java&gt;
    &lt;/target&gt;
</pre>

This defines an Ant target called "run". It first runs the "all" target, 
which compiles the hello world demo code. It then executes the class 
<code>demo.sphinx.hellodigits.HelloDigits</code>
which contains a main() method as you have seen above. It also forks a separate
process to run the Hello Digits demo, with a maximum heap size of 128MB.
The classpath is defined by the Ant path ID "run.classpath", which is defined
at the top of the build.xml file. Finally, the configuration file 
<code>hellodigits.config.xml</code> is fed as an argument to the HelloDigits
class.

<p>
This concludes the walkthrough of the simple Hello Digits example.
To recap the following things were done to create the Hello Digits demo:
<ol>
   <li>Write the code - HelloDigits.java</li>
   <li>Create a configuration file that specifies how Sphinx-4 should be
       configured - hellodigits.config.xml</li>
   <li>Create the build.xml file that include the Ant run targets</li>
</ol>
</p>

  </font>
</body>

</html>

